{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enDVf5tlxChH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "23ghxQdB5rfD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6ib43f8J8bDd",
        "outputId": "dfc93002-e927-4804-8728-3ca9f9caff10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j4gExIgu9i_o"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "img = image.load_img(\"Website Screenshots.v1-raw.tensorflow/iam_words/words/a01/a01-011/a01-011-04-05.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BuTylZXu91z_",
        "outputId": "69523090-7ed5-4116-bb75-9002b9ec2b8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=75x37>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tDJlx-yN5ySf"
      },
      "outputs": [],
      "source": [
        "image_dir = 'Website Screenshots.v1-raw.tensorflow/iam_words/words'  # Update this path\n",
        "labels_path = 'Website Screenshots.v1-raw.tensorflow/iam_words/words.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "# Define the CTC loss function\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "# Define the model\n",
        "input_data = Input(name='input', shape=(128, 32, 1), dtype='float32')\n",
        "conv_1 = Conv2D(32, (3, 3), padding='same', activation='relu')(input_data)\n",
        "pool_1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv_1)\n",
        "conv_2 = Conv2D(64, (3, 3), padding='same', activation='relu')(pool_1)\n",
        "pool_2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv_2)\n",
        "# Correct reshape dimensions\n",
        "reshape = Reshape(target_shape=(32, 512))(pool_2)\n",
        "\n",
        "# Bidirectional LSTM layers\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True))(reshape)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True))(blstm_1)\n",
        "dense = Dense(80, activation='softmax')(blstm_2)\n",
        "\n",
        "# Define the inputs for CTC\n",
        "labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "# Lambda layer for CTC loss\n",
        "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([dense, labels, input_length, label_length])\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)\n",
        "model.compile(optimizer='adam', loss={'ctc': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iXp56_t7IwI",
        "outputId": "3423960c-2ce5-4845-d1c0-e5198baaef26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 128, 32, 1)]         0         []                            \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 128, 32, 32)          320       ['input[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 64, 16, 32)           0         ['conv2d_2[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 64, 16, 64)           18496     ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 32, 8, 64)            0         ['conv2d_3[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 32, 512)              0         ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  (None, 32, 256)              656384    ['reshape_1[0][0]']           \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirecti  (None, 32, 256)              394240    ['bidirectional_2[0][0]']     \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 32, 80)               20560     ['bidirectional_3[0][0]']     \n",
            "                                                                                                  \n",
            " the_labels (InputLayer)     [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_length (InputLayer)   [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " label_length (InputLayer)   [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " ctc (Lambda)                (None, 1)                    0         ['dense[0][0]',               \n",
            "                                                                     'the_labels[0][0]',          \n",
            "                                                                     'input_length[0][0]',        \n",
            "                                                                     'label_length[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1090000 (4.16 MB)\n",
            "Trainable params: 1090000 (4.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that labels are within the valid range\n",
        "num_classes = 80  # Assuming 80 classes including the blank label\n",
        "max_label_value = num_classes - 1\n",
        "\n",
        "# Example input data\n",
        "images = np.random.rand(32, 128, 32, 1)  # Replace with actual image data\n",
        "\n",
        "# Create random label sequences ensuring they are within valid range\n",
        "label_sequences = [np.random.randint(1, max_label_value, np.random.randint(1, 16)).tolist() for _ in range(32)]\n",
        "\n",
        "# Pad the label sequences with a null label (e.g., max_label_value) to make them of equal length\n",
        "max_label_length = max(len(seq) for seq in label_sequences)\n",
        "label_sequences_padded = np.array([seq + [max_label_value] * (max_label_length - len(seq)) for seq in label_sequences])\n",
        "\n",
        "# Input and label lengths\n",
        "input_lengths = np.array([32] * 32)  # Assuming the input length is 32 for all sequences\n",
        "label_lengths = np.array([len(seq) for seq in label_sequences])  # Actual lengths of label sequences\n",
        "\n",
        "# Model fitting\n",
        "model.fit(\n",
        "    x=[images, label_sequences_padded, input_lengths, label_lengths],\n",
        "    y=np.zeros(len(images)),  # CTC requires dummy y data\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqLkynMpC1f1",
        "outputId": "a8dc8ec1-3c9d-4b66-b241-59b063f456c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 14s 14s/step - loss: 117.5680 - val_loss: 99.6457\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 1s 939ms/step - loss: 100.7787 - val_loss: 76.8096\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 74.1692 - val_loss: 54.6572\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 1s 913ms/step - loss: 46.8461 - val_loss: 58.2186\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 1s 984ms/step - loss: 44.9362 - val_loss: 65.0722\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 49.1050 - val_loss: 65.5870\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 49.0915 - val_loss: 62.6296\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 46.7074 - val_loss: 58.5292\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 43.7399 - val_loss: 55.1237\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 41.6880 - val_loss: 53.7252\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 41.5639 - val_loss: 54.0863\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 42.7069 - val_loss: 54.5213\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 43.2314 - val_loss: 54.4476\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 42.7320 - val_loss: 54.2406\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 41.7895 - val_loss: 54.3247\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 40.9851 - val_loss: 54.8437\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 40.5842 - val_loss: 55.6730\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 40.5607 - val_loss: 56.5606\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 40.7356 - val_loss: 57.2698\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 40.9162 - val_loss: 57.6657\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 40.9818 - val_loss: 57.7297\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 40.9034 - val_loss: 57.5308\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 40.7230 - val_loss: 57.1833\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 40.5165 - val_loss: 56.8050\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 40.3581 - val_loss: 56.4841\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 40.2906 - val_loss: 56.2594\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 40.3103 - val_loss: 56.1238\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 40.3730 - val_loss: 56.0475\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 1s 914ms/step - loss: 40.4219 - val_loss: 56.0050\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 40.4184 - val_loss: 55.9905\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 1s 948ms/step - loss: 40.3575 - val_loss: 56.0150\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 1s 986ms/step - loss: 40.2628 - val_loss: 56.0927\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 40.1687 - val_loss: 56.2256\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 40.1028 - val_loss: 56.3971\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 40.0747 - val_loss: 56.5749\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 40.0760 - val_loss: 56.7211\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 40.0880 - val_loss: 56.8040\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 40.0914 - val_loss: 56.8083\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 40.0752 - val_loss: 56.7406\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 40.0387 - val_loss: 56.6253\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 39.9891 - val_loss: 56.4864\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 39.9399 - val_loss: 56.3471\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 39.9016 - val_loss: 56.2283\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 39.8775 - val_loss: 56.1430\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 39.8647 - val_loss: 56.1012\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 39.8552 - val_loss: 56.1088\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 39.8389 - val_loss: 56.1512\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 39.8128 - val_loss: 56.2197\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 39.7769 - val_loss: 56.3116\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 39.7372 - val_loss: 56.4303\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ee532d83d30>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a file\n",
        "model.save('handwritten_text_recognition_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MlI4OLDDml3",
        "outputId": "a3e6de38-5294-4299-f967-65827563bac7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Save the entire model to a file\n",
        "model.save('handwritten_text_recognition_model.h5')\n",
        "\n",
        "# Download the saved model file\n",
        "files.download('handwritten_text_recognition_model.h5')"
      ],
      "metadata": {
        "id": "ut45bBZfZm_m",
        "outputId": "7a9e21b5-9d2a-42f0-ba87-8f84a63406cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_764ffd4d-030e-4cd8-aed5-951f28995fe0\", \"handwritten_text_recognition_model.h5\", 13170768)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have the following data\n",
        "# images: array of input images\n",
        "# label_sequences_padded: array of padded label sequences\n",
        "# input_lengths: array of input lengths\n",
        "# label_lengths: array of label lengths\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "images_train, images_test, label_sequences_padded_train, label_sequences_padded_test, input_lengths_train, input_lengths_test, label_lengths_train, label_lengths_test = train_test_split(\n",
        "    images, label_sequences_padded, input_lengths, label_lengths, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='handwritten_text_recognition_model_checkpoint.h5',\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    monitor='val_loss',   # Monitor the validation loss\n",
        "    mode='min',           # Save the model with minimum validation loss\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Model fitting with checkpoint callback\n",
        "history = model.fit(\n",
        "    x=[images_train, label_sequences_padded_train, input_lengths_train, label_lengths_train],\n",
        "    y=np.zeros(len(images_train)),  # CTC requires dummy y data\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    validation_data=([images_test, label_sequences_padded_test, input_lengths_test, label_lengths_test], np.zeros(len(images_test))),\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss = model.evaluate(\n",
        "    x=[images_test, label_sequences_padded_test, input_lengths_test, label_lengths_test],\n",
        "    y=np.zeros(len(images_test))  # CTC requires dummy y data\n",
        ")\n",
        "\n",
        "print(f'Test loss: {test_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOIIYjIbD9Tc",
        "outputId": "ffe748ff-4476-415c-fe9f-383cc25b9fdd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 43.2027\n",
            "Epoch 1: val_loss improved from inf to 43.98764, saving model to handwritten_text_recognition_model_checkpoint.h5\n",
            "1/1 [==============================] - 2s 2s/step - loss: 43.2027 - val_loss: 43.9876\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 43.1538\n",
            "Epoch 2: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 1s/step - loss: 43.1538 - val_loss: 44.1197\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 43.0685\n",
            "Epoch 3: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 1s/step - loss: 43.0685 - val_loss: 44.2906\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.9531\n",
            "Epoch 4: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 946ms/step - loss: 42.9531 - val_loss: 44.4910\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.8404\n",
            "Epoch 5: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 42.8404 - val_loss: 44.6763\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.7272\n",
            "Epoch 6: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 42.7272 - val_loss: 44.8743\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.6177\n",
            "Epoch 7: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 42.6177 - val_loss: 45.0948\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.5246\n",
            "Epoch 8: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 42.5246 - val_loss: 45.3206\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.4471\n",
            "Epoch 9: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 42.4471 - val_loss: 45.5106\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.3709\n",
            "Epoch 10: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 42.3709 - val_loss: 45.6520\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.3048\n",
            "Epoch 11: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 42.3048 - val_loss: 45.7982\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.2474\n",
            "Epoch 12: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 42.2474 - val_loss: 45.9817\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.1907\n",
            "Epoch 13: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 42.1907 - val_loss: 46.1858\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.1426\n",
            "Epoch 14: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 42.1426 - val_loss: 46.3762\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.1002\n",
            "Epoch 15: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 42.1002 - val_loss: 46.5455\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.0552\n",
            "Epoch 16: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 42.0552 - val_loss: 46.7115\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 42.0165\n",
            "Epoch 17: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 42.0165 - val_loss: 46.8698\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.9711\n",
            "Epoch 18: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 635ms/step - loss: 41.9711 - val_loss: 47.0149\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.9193\n",
            "Epoch 19: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 41.9193 - val_loss: 47.1407\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.8677\n",
            "Epoch 20: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 41.8677 - val_loss: 47.2295\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.8112\n",
            "Epoch 21: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 41.8112 - val_loss: 47.2829\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.7535\n",
            "Epoch 22: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 41.7535 - val_loss: 47.3194\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.6991\n",
            "Epoch 23: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 41.6991 - val_loss: 47.3511\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.6445\n",
            "Epoch 24: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 937ms/step - loss: 41.6445 - val_loss: 47.3820\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.5887\n",
            "Epoch 25: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 41.5887 - val_loss: 47.4068\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.5328\n",
            "Epoch 26: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 970ms/step - loss: 41.5328 - val_loss: 47.4121\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.4762\n",
            "Epoch 27: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 41.4762 - val_loss: 47.3801\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.4233\n",
            "Epoch 28: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 1s/step - loss: 41.4233 - val_loss: 47.4588\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.3674\n",
            "Epoch 29: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 795ms/step - loss: 41.3674 - val_loss: 47.4798\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.3137\n",
            "Epoch 30: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 41.3137 - val_loss: 47.4940\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.2623\n",
            "Epoch 31: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 41.2623 - val_loss: 47.5215\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.2116\n",
            "Epoch 32: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 41.2116 - val_loss: 47.5690\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.1579\n",
            "Epoch 33: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 41.1579 - val_loss: 47.6391\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.1031\n",
            "Epoch 34: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 41.1031 - val_loss: 47.7260\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 41.0508\n",
            "Epoch 35: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 41.0508 - val_loss: 47.8004\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.9984\n",
            "Epoch 36: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 40.9984 - val_loss: 47.8533\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.9438\n",
            "Epoch 37: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 40.9438 - val_loss: 47.9184\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.8900\n",
            "Epoch 38: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 40.8900 - val_loss: 48.0148\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.8352\n",
            "Epoch 39: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 40.8352 - val_loss: 48.1423\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.7790\n",
            "Epoch 40: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 40.7790 - val_loss: 48.2813\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.7241\n",
            "Epoch 41: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 40.7241 - val_loss: 48.3810\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.6673\n",
            "Epoch 42: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 40.6673 - val_loss: 48.4528\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.6102\n",
            "Epoch 43: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 40.6102 - val_loss: 48.5445\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.5539\n",
            "Epoch 44: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 40.5539 - val_loss: 48.6749\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.4950\n",
            "Epoch 45: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 40.4950 - val_loss: 48.8177\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.4367\n",
            "Epoch 46: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 40.4367 - val_loss: 48.9223\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.3760\n",
            "Epoch 47: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 40.3760 - val_loss: 49.0089\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.3168\n",
            "Epoch 48: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 647ms/step - loss: 40.3168 - val_loss: 49.1308\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.2558\n",
            "Epoch 49: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 799ms/step - loss: 40.2558 - val_loss: 49.2719\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 40.1939\n",
            "Epoch 50: val_loss did not improve from 43.98764\n",
            "1/1 [==============================] - 1s 834ms/step - loss: 40.1939 - val_loss: 49.3736\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 49.3736\n",
            "Test loss: 49.3736457824707\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}